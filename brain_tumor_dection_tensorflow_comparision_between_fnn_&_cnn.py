# -*- coding: utf-8 -*-
"""Brain Tumor Dection- TensorFlow Comparision Between FNN & CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18ObaWKdxkfs3Y0P6fsBcFiEZWTIfx5e7

## BRAIN TUMOR DETECTION WITH TENSORFLOW (COMPARISION BETWEEN FNN AND CNN)

# R.ANANDHA RAGAVEN
# CSE - A

## FEED FORWARD NEURAL NETWORK
"""

from zipfile import ZipFile
file_name = "brain-tumor .zip"

with  ZipFile(file_name, 'r') as zip:
  zip.extractall()
  print('Done')

#Load libraries
import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import torch
import glob
import pathlib

#checking for device
device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(device)

import pathlib
import numpy as np
data_dir = pathlib.Path("/content/brain-tumor /train") 
class_names = np.array(sorted([item.name for item in data_dir.glob('*')])) 
class_names = class_names[1:]
print(class_names)

# Let's visuvalize our images
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import random

def view_random_image(target_dir, target_class):
  # Setup the target directory
  target_folder = target_dir+target_class

  # Get a random image path
  random_image = random.sample(os.listdir(target_folder), 1)

  
  img = mpimg.imread(target_folder + "/" + random_image[0])
  plt.imshow(img)
  plt.title(target_class)
  plt.axis("off");

  print(f"Image shape: {img.shape}") # show the shape  of the img

  return img

img = view_random_image(target_dir="brain-tumor /train/",
                        target_class="Brain Tumor")

img = view_random_image(target_dir="brain-tumor /train/",
                        target_class="Healthy")

img

img.shape

import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Create ImageDataGenerator training instance with data augmentation (horizontal flip is True)
train_datagen = ImageDataGenerator(rescale=1/255.,
                                   horizontal_flip=True) 

# Create ImageDataGenerator test instance without data augmentation
test_datagen = ImageDataGenerator(rescale=1/255.)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Set the seed
tf.random.set_seed(42)

# Preprocess data (get all of the pixel values between 1 and 0, also called scaling/normalization)



train_dir = "brain-tumor /train/"
test_dir = "brain-tumor /test/"

# Import data from directories and turn it into batches
train_data = train_datagen.flow_from_directory(train_dir,
                                               batch_size=32, # number of images to process at a time 
                                               target_size=(150, 150), # convert all images to be 150 x 150
                                               class_mode="binary", # type of problem we're working on (binary class classification predict whether it is Healthy or Cancer)
                                               seed=42)

valid_data= test_datagen.flow_from_directory(test_dir,
                                               batch_size=32,
                                               target_size=(150, 150),
                                               class_mode="binary",
                                               seed=42)

"""### TRYING DIFFERENT OPTIMIZER

### SGD
"""

tf.random.set_seed(42)


model_a = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(150, 150, 3)), 
  tf.keras.layers.Dense(4, activation='relu'),
  tf.keras.layers.Dense(4, activation='relu'),
  tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model_a.compile(loss='binary_crossentropy',
              optimizer=tf.keras.optimizers.SGD(),
              metrics=["accuracy"])

# Fit the model
history_a = model_a.fit(train_data, # use same training data created above
                        epochs=5,
                        steps_per_epoch=len(train_data),
                        validation_data=valid_data, # use same validation data created above
                        validation_steps=len(valid_data))

"""### ADAM"""

tf.random.set_seed(42)


model_b = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(150, 150, 3)), 
  tf.keras.layers.Dense(4, activation='relu'),
  tf.keras.layers.Dense(4, activation='relu'),
  tf.keras.layers.Dense(1, activation='sigmoid')
])


model_b.compile(loss='binary_crossentropy',
              optimizer=tf.keras.optimizers.Adam(),
              metrics=["accuracy"])


history_b = model_b.fit(train_data, 
                        epochs=5,
                        steps_per_epoch=len(train_data),
                        validation_data=valid_data,
                        validation_steps=len(valid_data))

"""### INCREASING THE HIDDEN LAYER"""

tf.random.set_seed(42)


model_c = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(150 , 150, 3)), 
  tf.keras.layers.Dense(4, activation='relu'),
  tf.keras.layers.Dense(4, activation='relu'),
  tf.keras.layers.Dense(4, activation='relu'),
  tf.keras.layers.Dense(4, activation='relu'),
  tf.keras.layers.Dense(1, activation='sigmoid')
])


model_c.compile(loss='binary_crossentropy',
              optimizer=tf.keras.optimizers.Adam(),
              metrics=["accuracy"])


history_c = model_c.fit(train_data, 
                        epochs=5,
                        steps_per_epoch=len(train_data),
                        validation_data=valid_data, 
                        validation_steps=len(valid_data))

"""# Increasing epoch"""

tf.random.set_seed(42)


model_d = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(150, 150, 3)), 
  tf.keras.layers.Dense(4, activation='relu'),
  tf.keras.layers.Dense(4, activation='relu'),
  tf.keras.layers.Dense(4, activation='relu'),
  tf.keras.layers.Dense(4, activation='relu'),
  tf.keras.layers.Dense(1, activation='sigmoid')
])


model_d.compile(loss='binary_crossentropy',
              optimizer=tf.keras.optimizers.Adam(),
              metrics=["accuracy"])


history_d = model_d.fit(train_data, 
                        epochs=10,
                        steps_per_epoch=len(train_data),
                        validation_data=valid_data, 
                        validation_steps=len(valid_data))

# creating the dataset
data = {'Changing optimizer(SGD)':54.83, 'Changing optimizer(Adam)':54.83, 'Increasing Hidden layer':83.52,
        'Increasing epochs':89.33}
courses = list(data.keys())
values = list(data.values())
  
fig = plt.figure(figsize = (20,5))
 
# creating the bar plot
plt.bar(courses, values, color ='blue',
        width = 0.4)
 
plt.xlabel("PARAMETER TUNING")
plt.ylabel("TEST ACCURAY")
plt.title("FEED FORWARD NEURAL NETWORK(TRAIN ACCURACY)")
plt.show()

# creating the dataset
data = {'Changing optimizer(SGD)':51.24, 'Changing optimizer(Adam)':51.24, 'Increasing Hidden layer':84.30,
        'Increasing epochs':88.43}
courses = list(data.keys())
values = list(data.values())
  
fig = plt.figure(figsize = (20,5))
 
# creating the bar plot
plt.bar(courses, values, color ='blue',
        width = 0.4)
 
plt.xlabel("PARAMETER TUNING")
plt.ylabel("TEST ACCURAY")
plt.title("FEED FORWARD NEURAL NETWORK(TEST ACCURACY)")
plt.show()

# Plot the validation and training data separately
def plot_loss_curves(history):
  """
  Returns separate loss curves for training and validation metrics.
  """ 
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']

  epochs = range(len(history.history['loss']))

  # Plot loss
  plt.plot(epochs, loss, label='training_loss')
  plt.plot(epochs, val_loss, label='val_loss')
  plt.title('Loss')
  plt.xlabel('Epochs')
  plt.legend()

  # Plot accuracy
  plt.figure()
  plt.plot(epochs, accuracy, label='training_accuracy')
  plt.plot(epochs, val_accuracy, label='val_accuracy')
  plt.title('Accuracy')
  plt.xlabel('Epochs')
  plt.legend();

plot_loss_curves(history_a)

plot_loss_curves(history_b)

plot_loss_curves(history_c)

plot_loss_curves(history_d)

"""## Increasing Hidden layer and epochs seems like it shown promising results - Test Accuracy of ~ 88%

## Changing the optimizer only gave an test accuracy around ~54%

## But let's look for CNN

## CNN
"""

model_1 = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(filters=10, 
                         kernel_size=3, # can also be (3, 3)
                         activation="relu", 
                         input_shape=(150, 150, 3)), # first layer specifies input shape (height, width, colour channels)


  tf.keras.layers.Conv2D(10, 3, activation="relu"),
  tf.keras.layers.MaxPool2D(pool_size=2, # pool_size can also be (2, 2)
                            padding="valid"), 
  tf.keras.layers.Conv2D(10, 3, activation="relu"),
  tf.keras.layers.Conv2D(10, 3, activation="relu"),
  
  tf.keras.layers.MaxPool2D(2),


  



  tf.keras.layers.Flatten(),


  tf.keras.layers.Dense(1, activation="sigmoid") # binary activation output
])

# Compile the model
model_1.compile(loss="binary_crossentropy",
              optimizer=tf.keras.optimizers.Adam(),
              metrics=["accuracy"])

# Fit the model
history_1 = model_1.fit(train_data,
                        epochs=10,
                        steps_per_epoch=len(train_data),
                        validation_data=valid_data,
                        validation_steps=len(valid_data))

# Check out the layers in our model
model_1.summary()

# Plot the validation and training data separately
def plot_loss_curves(history):
  """
  Returns separate loss curves for training and validation metrics.
  """ 
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']

  epochs = range(len(history.history['loss']))

  # Plot loss
  plt.plot(epochs, loss, label='training_loss')
  plt.plot(epochs, val_loss, label='val_loss')
  plt.title('Loss')
  plt.xlabel('Epochs')
  plt.legend()

  # Plot accuracy
  plt.figure()
  plt.plot(epochs, accuracy, label='training_accuracy')
  plt.plot(epochs, val_accuracy, label='val_accuracy')
  plt.title('Accuracy')
  plt.xlabel('Epochs')
  plt.legend();

# Check out the loss curves of model_1
plot_loss_curves(history_1)

# Evaluate on the test data
model_1.evaluate(valid_data )

def view_random_image(target_dir, target_class):
  # Setup target directory (we'll view images from here)
  target_folder = target_dir+target_class

  # Get a random image path
  random_image = random.sample(os.listdir(target_folder), 1)

  # Read in the image and plot it using matplotlib
  img = mpimg.imread(target_folder + "/" + random_image[0])
  plt.imshow(img)
  plt.title(target_class)
  plt.axis("off");

  print(f"Image shape: {img.shape}") # show the shape of the image

  return img

# Create a function to import an image and resize it to be able to be used with our model
def load_and_prep_image(filename, img_shape=150):
  """
  Reads an image from filename, turns it into a tensor
  and reshapes it to (img_shape, img_shape, colour_channel).
  """
  # Read in target file (an image)
  img = tf.io.read_file(filename)

  # Decode the read file into a tensor & ensure 3 colour channels 
  # (our model is trained on images with 3 colour channels and sometimes images have 4 colour channels)
  img = tf.image.decode_image(img, channels=3)

  # Resize the image (to the same size our model was trained on)
  img = tf.image.resize(img, size = [img_shape, img_shape])

  # Rescale the image (get all values between 0 and 1)
  img = img/255.
  return img

def pred_and_plot(model, filename, class_names):
  """
  Imports an image located at filename, makes a prediction on it with
  a trained model and plots the image with the predicted class as the title.
  """
  # Import the target image and preprocess it
  img = load_and_prep_image(filename)

  # Make a prediction
  pred = model.predict(tf.expand_dims(img, axis=0))

  # Get the predicted class
  pred_class = class_names[int(tf.round(pred)[0][0])]

  # Plot the image and predicted class
  plt.imshow(img)
  plt.title(f"Prediction: {pred_class}")
  plt.axis(False);

# Test our model on a custom image
pred_and_plot(model_1, "/content/brain-tumor /pred/Cancer (2415).jpg", class_names)

pred_and_plot(model_1, "/content/brain-tumor /pred/Not Cancer  (2075).jpg", class_names)

"""
# IT SEEMS THAT CNN HAS OUT PERFORMED FNN 
# ACCURACY
# FNN - ~88% (model_d)
# CNN-  ~97% (model_1)

"""